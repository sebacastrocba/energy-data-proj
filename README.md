# An√°lisis de Precios de Combustibles en Argentina

![CI](https://github.com/sebacastrocba/energy-data-proj/workflows/CI/badge.svg)
[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/release/python-3120/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## Descripci√≥n del Proyecto

Este proyecto implementa un pipeline de datos (ETL) completo que extrae, transforma y carga informaci√≥n sobre precios de combustibles en Argentina, combin√°ndola con datos del precio internacional del petr√≥leo Brent y cotizaciones USD/ARS (oficial y blue). El objetivo es analizar la correlaci√≥n entre los precios locales, el mercado internacional de petr√≥leo y la evoluci√≥n del tipo de cambio.

**Caso de uso:** Permite a analistas y tomadores de decisiones entender c√≥mo fluct√∫an los precios de combustibles en Argentina en relaci√≥n con el precio del petr√≥leo Brent y el tipo de cambio, identificando patrones temporales y geogr√°ficos.

**‚ö†Ô∏è Nota sobre disponibilidad de datos:** Los datos de combustibles de la Secretar√≠a de Energ√≠a est√°n disponibles √∫nicamente desde 2025 en adelante, debido a que las tablas correspondientes al per√≠odo 2022-2024 no se encuentran disponibles en la fuente oficial al momento del desarrollo del proyecto.

## Caracter√≠sticas Principales

- ‚úÖ **Extracci√≥n de datos** de m√∫ltiples fuentes (Secretar√≠a de Energ√≠a, Yahoo Finance, Bluelytics API)
- ‚úÖ **Transformaci√≥n y limpieza** de datos con pandas y numpy
- ‚úÖ **Carga a PostgreSQL** con Docker Compose (schemas staging y analytics)
- ‚úÖ **Carga a AWS Redshift** para producci√≥n (opcional)
- ‚úÖ **Orquestaci√≥n con Apache Airflow** con carga paralela a m√∫ltiples destinos
- ‚úÖ **Pipeline ETL automatizado** con ejecuci√≥n diaria programada
- ‚úÖ **An√°lisis exploratorio** con Jupyter notebooks
- ‚úÖ **Tests automatizados** con pytest y pytest-cov
- ‚úÖ **Type checking** con mypy
- ‚úÖ **Code quality** con black y flake8
- ‚úÖ **Gesti√≥n de dependencias** con Poetry
- ‚úÖ **Context managers** para manejo seguro de conexiones DB

## üöÄ Inicio R√°pido

### Requisitos Previos

- Docker y Docker Compose instalados
- Python 3.12+
- Poetry (recomendado) o pip

### Instalaci√≥n

```bash
# 1. Clonar el repositorio
git clone https://github.com/sebacastrocba/energy-data-proj.git
cd fuel_price_project

# 2. Crear archivo de configuraci√≥n
cp .env.example .env
# Edita .env si necesitas cambiar credenciales

# 3. Levantar PostgreSQL con Docker
docker-compose up -d

# 4. Instalar dependencias Python con Poetry
poetry install
```

El archivo `sql/init.sql` se ejecuta autom√°ticamente al crear el contenedor de PostgreSQL por primera vez, inicializando los schemas y tablas necesarias.

## üìä Uso del Pipeline

### Pipeline ETL Paso a Paso

El pipeline ETL se ejecuta en tres pasos secuenciales:

**Paso 1: Extracci√≥n** - Descarga datos de APIs y los guarda en `data/raw/` como CSV
```bash
poetry run python src/fuel_price/extract.py
```
- Descarga precios de Brent desde Yahoo Finance
- Descarga cotizaciones USD/ARS desde Bluelytics API
- Extrae datos de combustibles de la Secretar√≠a de Energ√≠a
- Guarda archivos CSV en `data/raw/`

**Paso 2: Transformaci√≥n** - Lee CSVs, limpia y agrega datos, genera Parquets en `data/processed/`
```bash
poetry run python src/fuel_price/transform.py
```
- Lee archivos CSV de `data/raw/`
- Limpia y valida datos
- Genera agregaciones mensuales
- Guarda archivos Parquet en `data/processed/`

**Paso 3: Carga** - Lee Parquets y carga a PostgreSQL
```bash
poetry run python src/fuel_price/load.py
```
- Lee archivos Parquet de `data/processed/`
- Carga datos a tablas `staging.*` (datos limpios)
- Carga agregaciones a tablas `analytics.*` (datos mensuales)

### Usar como Librer√≠a

```python
from fuel_price.extract import extract_brent_prices, extract_dolar_bluelytics, extract_fuel_prices
from fuel_price.transform import (
    clean_brent_price, 
    agg_brent_price_for_analytics,
    clean_fuel_price, 
    agg_fuel_price_for_analytics,
    clean_dollar_price,
    agg_dollar_price_for_analytics
)
from fuel_price.load import (
    load_brent_to_staging, 
    load_brent_to_analytics,
    load_fuel_to_staging,
    load_fuel_to_analytics,
    load_dollar_to_staging,
    load_dollar_to_analytics
)

# Extraer
brent_raw = extract_brent_prices()
fuel_raw = extract_fuel_prices()
usd_raw = extract_dolar_bluelytics()

# Transformar
brent_clean = clean_brent_price(brent_raw)
brent_analytics = agg_brent_price_for_analytics(brent_clean)

# Cargar a PostgreSQL
load_brent_to_staging(brent_clean, truncate=True)
load_brent_to_analytics(brent_analytics, truncate=True)
```


## üóÇÔ∏è Estructura de Base de Datos

### Schemas Creados

- **`staging`**: Datos crudos o m√≠nimamente procesados
- **`analytics`**: Datos transformados y agregados a nivel mensual

### Tablas Staging

| Tabla | Descripci√≥n | Campos Principales |
|-------|-------------|-------------------|
| `staging.brent_price` | Precios diarios del petr√≥leo Brent | `date`, `brent_price` |
| `staging.fuel_prices` | Precios de combustibles por estaci√≥n | `periodo`, `provincia`, `producto`, `precio_surtidor`, `volumen` |
| `staging.usd_ars_rates` | Cotizaci√≥n USD/ARS (oficial y blue) | `date`, `source`, `value_buy`, `value_sell` |

### Tablas Analytics (Agregaciones Mensuales)

| Tabla | Descripci√≥n | Agregaciones |
|-------|-------------|--------------|
| `analytics.brent_price_monthly` | Brent agregado mensualmente | Promedio, min, max, desviaci√≥n est√°ndar |
| `analytics.fuel_prices_monthly` | Combustibles por mes/provincia/producto | Promedio ponderado por volumen |
| `analytics.usd_ars_rates_monthly` | USD/ARS mensual con brecha cambiaria | Promedio blue, oficial, brecha % |

## üîç Explorar la Base de Datos

### Conectar con psql

```bash
# Usando las credenciales por defecto
PGPASSWORD=fuel_password psql -h localhost -p 5432 -U fuel_user -d fuel_prices_db
```

### Consultas √ötiles

```sql
-- Ver schemas
\dn

-- Ver tablas de staging
\dt staging.*

-- Ver tablas de analytics
\dt analytics.*

-- Ver estructura de una tabla
\d staging.brent_price

-- Consultar √∫ltimos precios de Brent
SELECT * FROM staging.brent_price ORDER BY date DESC LIMIT 10;

-- Ver agregaci√≥n mensual de combustibles por provincia
SELECT year, month, provincia, producto, avg_price 
FROM analytics.fuel_prices_monthly 
WHERE year = 2024 AND provincia = 'BUENOS AIRES'
ORDER BY month DESC, producto
LIMIT 20;

-- Comparar brecha cambiaria mensual
SELECT year, month, avg_blue, avg_oficial, brecha_pct
FROM analytics.usd_ars_rates_monthly
ORDER BY year DESC, month DESC
LIMIT 12;
```

## üê≥ Comandos Docker √ötiles

```bash
# Ver estado del contenedor
docker-compose ps

# Ver logs
docker-compose logs -f

# Detener (datos se mantienen)
docker-compose stop

# Reiniciar
docker-compose restart

# Eliminar todo (BORRA DATOS)
docker-compose down -v
```

## üß™ Testing

El proyecto incluye tests automatizados para las funciones de extracci√≥n y transformaci√≥n.

### Ejecutar todos los tests

```bash
poetry run pytest
```

### Ejecutar tests con cobertura

```bash
poetry run pytest --cov=src/fuel_price --cov-report=html
```

El reporte HTML se genera en `htmlcov/index.html`.

### Ejecutar tests espec√≠ficos

```bash
# Solo tests de extracci√≥n
poetry run pytest tests/test_extract.py

# Solo tests de transformaci√≥n
poetry run pytest tests/test_transform.py

# Ejecutar un test espec√≠fico
poetry run pytest tests/test_transform.py::test_agg_brent_price_calculates_monthly_average
```

### Verificaci√≥n de tipos con mypy

```bash
poetry run mypy src/fuel_price
```

## üìÅ Estructura del Proyecto

```
fuel_price_project/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ fuel_price/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ config.py            # Configuraci√≥n global y constantes
‚îÇ       ‚îú‚îÄ‚îÄ extract.py           # Extracci√≥n de datos de APIs
‚îÇ       ‚îú‚îÄ‚îÄ transform.py         # Transformaci√≥n y limpieza de datos
‚îÇ       ‚îú‚îÄ‚îÄ load.py              # Carga a PostgreSQL con context managers
‚îÇ       ‚îú‚îÄ‚îÄ load_redshift.py     # Carga a AWS Redshift
‚îÇ       ‚îî‚îÄ‚îÄ get_price_data_SE.py # Extractor de datos de Secretar√≠a de Energ√≠a
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_extract.py          # Tests de extracci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ test_transform.py        # Tests de transformaci√≥n
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ fuel_price_dag.py        # DAG de Airflow con carga paralela
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ 00_analisis_exploratorio.ipynb  # An√°lisis exploratorio de datos
‚îú‚îÄ‚îÄ sql/
‚îÇ   ‚îî‚îÄ‚îÄ init.sql                 # Schema SQL (staging + analytics)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                     # Datos originales (gitignored)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2025_plus.zip        # Archivo descargado de SE (2025+)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2025_plus.done       # Marca de descarga completada
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ brent_prices.csv     # Precios hist√≥ricos de Brent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ usd_ars_bluelytics.csv  # Cotizaciones USD/ARS
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ precios_eess_completo.csv  # Datos consolidados de combustibles
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ precios_eess_2025_en_adelante.accdb  # Base Access original SE
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ precios_eess_2025_en_adelante_public_vi_access_eess_2025_en_adelante.csv
‚îÇ   ‚îî‚îÄ‚îÄ processed/               # Datos procesados (gitignored)
‚îÇ       ‚îú‚îÄ‚îÄ brent_price_cleaned.parquet    # Precios Brent limpios
‚îÇ       ‚îú‚îÄ‚îÄ brent_price_monthly.parquet    # Agregaci√≥n mensual Brent
‚îÇ       ‚îú‚îÄ‚îÄ fuel_price_cleaned.parquet     # Precios combustibles limpios
‚îÇ       ‚îú‚îÄ‚îÄ fuel_price_aggregated.parquet  # Agregaci√≥n mensual combustibles
‚îÇ       ‚îú‚îÄ‚îÄ dollar_price_cleaned.parquet   # Cotizaciones limpias
‚îÇ       ‚îî‚îÄ‚îÄ dollar_price_aggregated.parquet  # Agregaci√≥n mensual USD/ARS
‚îú‚îÄ‚îÄ logs/                        # Logs de Airflow (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ scheduler/               # Logs del scheduler
‚îÇ   ‚îî‚îÄ‚îÄ dag_id=fuel_price_etl/   # Logs por ejecuci√≥n del DAG
‚îú‚îÄ‚îÄ docs/                        # Documentaci√≥n adicional
‚îú‚îÄ‚îÄ docker-compose.yml           # Configuraci√≥n de PostgreSQL y Airflow
‚îú‚îÄ‚îÄ Dockerfile.airflow           # Dockerfile para Airflow con dependencias
‚îú‚îÄ‚îÄ airflow.cfg                  # Configuraci√≥n de Airflow
‚îú‚îÄ‚îÄ airflow.db                   # Base de datos SQLite (legacy, no usado)
‚îú‚îÄ‚îÄ generate_fernet_key.py       # Script para generar FERNET_KEY
‚îú‚îÄ‚îÄ pyproject.toml               # Configuraci√≥n de Poetry y dependencias
‚îú‚îÄ‚îÄ poetry.lock                  # Lock file de Poetry
‚îú‚îÄ‚îÄ mypy.ini                     # Configuraci√≥n de mypy
‚îú‚îÄ‚îÄ .flake8                      # Configuraci√≥n de flake8
‚îú‚îÄ‚îÄ .env.example                 # Plantilla de variables de entorno
‚îú‚îÄ‚îÄ .env                         # Variables de entorno (gitignored)
‚îî‚îÄ‚îÄ README.md                    # Este archivo
```

## üìö Fuentes de Datos

- **Precios de Combustibles:** [Secretar√≠a de Energ√≠a de Argentina](http://res1104.se.gob.ar/) - Datos desde 2025 en adelante
  - ‚ö†Ô∏è **Nota:** Los datos hist√≥ricos 2022-2024 no est√°n disponibles en la fuente oficial al momento del desarrollo
  - Se utilizan √∫nicamente datos desde 2025 debido a la indisponibilidad de tablas anteriores
- **Precio del Brent:** [Yahoo Finance](https://finance.yahoo.com/) (s√≠mbolo: BZ=F) - Precios diarios
- **Tipo de Cambio USD/ARS:** [Bluelytics API](https://bluelytics.com.ar/#!/api) - Cotizaciones oficial y blue

## üîß Tecnolog√≠as Utilizadas

- **Lenguaje:** Python 3.12
- **Gesti√≥n de dependencias:** Poetry
- **Base de datos:** PostgreSQL 15 (Alpine)
- **Data Warehouse:** AWS Redshift (opcional)
- **Contenedores:** Docker y Docker Compose
- **Orquestaci√≥n:** Apache Airflow 2.10.3 (LocalExecutor)
- **An√°lisis de datos:** pandas, numpy
- **Testing:** pytest, pytest-cov, pytest-mock
- **Type checking:** mypy
- **Code formatting:** black, flake8
- **Formato de datos:** Parquet (PyArrow)
- **Conexi√≥n DB:** psycopg2 (PostgreSQL y Redshift)

## üõ†Ô∏è Soluci√≥n de Problemas

### PostgreSQL no inicia

```bash
# Ver logs del contenedor
docker-compose logs postgres

# Verificar que el contenedor est√© corriendo
docker-compose ps

# Reiniciar contenedor
docker-compose restart postgres

# Reiniciar desde cero (ELIMINA TODOS LOS DATOS)
docker-compose down -v
docker-compose up -d
```

### Error de conexi√≥n a PostgreSQL

```bash
# Verificar que el contenedor est√© corriendo
docker-compose ps

# Probar conexi√≥n desde Python
poetry run python -c "from src.fuel_price.load import test_connection; print('OK' if test_connection() else 'ERROR')"

# Verificar variables de entorno
cat .env
```

### Problemas con dependencias de Python

```bash
# Reinstalar dependencias
poetry install --no-cache

# Actualizar Poetry
poetry self update

# Verificar versi√≥n de Python
python --version  # Debe ser 3.12+
```

### Error al extraer datos de la Secretar√≠a de Energ√≠a

Los archivos `.accdb` requieren procesamiento especial. El script `get_price_data_SE.py` maneja la conversi√≥n autom√°ticamente.

```bash
# Ejecutar extractor manualmente
poetry run python src/fuel_price/get_price_data_SE.py
```

### Problemas con Airflow

**Airflow no inicia:**
```bash
# Ver logs del scheduler
docker logs airflow_scheduler

# Ver logs del webserver
docker logs airflow_webserver

# Verificar que la base de datos de Airflow est√© inicializada
docker exec airflow_scheduler airflow db check

# Reiniciar contenedores de Airflow
docker-compose restart airflow_scheduler airflow_webserver
```

**DAG no aparece en la UI:**
```bash
# Verificar que el archivo DAG est√© en el directorio correcto
ls -la dags/

# Verificar que no haya errores de sintaxis
poetry run python dags/fuel_price_dag.py

# Verificar desde Airflow
docker exec airflow_webserver airflow dags list | grep fuel

# Forzar actualizaci√≥n de DAGs
docker exec airflow_scheduler airflow dags reserialize
```

**Error en tarea load_redshift:**
```bash
# Verificar que las credenciales est√©n configuradas
cat .env | grep REDSHIFT

# Probar conexi√≥n a Redshift
poetry run python -c "from fuel_price.load_redshift import test_redshift_connection; test_redshift_connection()"

# Ver logs espec√≠ficos de la tarea
docker exec airflow_scheduler airflow tasks logs fuel_price_etl load_redshift <RUN_ID>
```

**DAG atascado o en estado indefinido:**
```bash
# Marcar tarea como fallida manualmente
docker exec airflow_scheduler airflow tasks clear fuel_price_etl -t <task_id> -s <start_date> -e <end_date>

# Reiniciar scheduler
docker-compose restart airflow_scheduler
```

**Problemas de permisos:**
```bash
# Verificar permisos de directorios
ls -la logs/ dags/

# Ajustar permisos si es necesario
chmod -R 755 logs/ dags/

# Verificar usuario de los contenedores
docker exec airflow_scheduler whoami
```

## üìù Notas Importantes

- **Puerto:** PostgreSQL corre en el puerto **5432** (puerto est√°ndar)
- **Credenciales:** Las credenciales por defecto son:
  - Usuario: `fuel_user`
  - Contrase√±a: `fuel_password`
  - Base de datos: `fuel_prices_db`
- **Variables de entorno:** Configura el archivo `.env` para personalizar las credenciales
- **Vol√∫menes:** Los datos de PostgreSQL persisten en un volumen Docker llamado `postgres_data`
- **Schemas:** 
  - `staging`: Para datos crudos o m√≠nimamente procesados
  - `analytics`: Para datos agregados y transformados
- **Inicializaci√≥n autom√°tica:** El archivo `sql/init.sql` se ejecuta autom√°ticamente al crear el contenedor por primera vez
- **Reinicializar:** Para borrar todos los datos y empezar de cero: `docker-compose down -v && docker-compose up -d`
- **Datos:** Los archivos CSV en `data/raw/` est√°n en `.gitignore` y no se suben al repositorio
- **Context managers:** El c√≥digo usa context managers para manejar conexiones a la base de datos de forma segura
- **‚ö†Ô∏è Limitaci√≥n de datos:** Los datos de combustibles de la Secretar√≠a de Energ√≠a est√°n disponibles √∫nicamente desde 2025, ya que las tablas correspondientes a 2022-2024 no est√°n disponibles en la fuente oficial

## ü§ñ Orquestaci√≥n con Apache Airflow

### Configuraci√≥n Inicial de Airflow

Airflow se ejecuta mediante Docker Compose. Para iniciar los servicios de Airflow:

```bash
# Iniciar todos los servicios (PostgreSQL + Airflow)
docker-compose up -d

# Verificar que los servicios est√©n corriendo
docker-compose ps
```

**Acceso a la UI:**
- URL: http://localhost:8080
- Usuario: `airflow`
- Contrase√±a: `airflow`

**Nota:** Las credenciales por defecto pueden configurarse en el archivo `docker-compose.yml`.

### Configuraci√≥n de Airflow

Airflow est√° configurado con:
- **Base de datos:** PostgreSQL dedicada para metadatos de Airflow (`airflow_postgres`)
- **Ejecutor:** LocalExecutor (permite paralelismo)
- **DAGs folder:** `./dags`
- **Logs folder:** `./logs`
- **Ejemplos:** Desactivados (`load_examples = False`)

**Componentes en Docker:**
- `airflow_webserver` - Interfaz web (puerto 8080)
- `airflow_scheduler` - Programador de tareas
- `airflow_init` - Inicializaci√≥n de la base de datos (se ejecuta una vez)
- `postgres-airflow` - Base de datos PostgreSQL para metadatos de Airflow
- `postgres-etl` - Base de datos PostgreSQL para datos del ETL (separada)

### DAG del Pipeline ETL con Carga Paralela

El DAG `fuel_price_etl` ejecuta el pipeline completo diariamente a las 2 AM con **carga paralela** a PostgreSQL y Redshift:

#### Estructura del Flujo

```
extract ‚Üí transform ‚Üí [load_postgres, load_redshift]
                           ‚Üì              ‚Üì
                      PostgreSQL      Redshift
                      (staging)     (producci√≥n)
```

#### Tareas del DAG

1. **extract** - Descarga datos de APIs externas (Brent, USD/ARS, combustibles)
2. **transform** - Limpia, valida y transforma los datos, genera archivos Parquet
3. **load_postgres** - Carga a PostgreSQL local (staging) ‚ö°
4. **load_redshift** - Carga a AWS Redshift (producci√≥n) ‚ö°

> ‚ö° Las tareas de carga se ejecutan **en paralelo**, optimizando el tiempo total del pipeline

#### Ventajas de la Carga Paralela

- **Optimizaci√≥n de tiempo:** Las cargas a PostgreSQL y Redshift ocurren simult√°neamente
- **Independencia:** Si una carga falla, la otra contin√∫a normalmente
- **Flexibilidad:** Puedes desactivar una carga sin afectar la otra
- **Escalabilidad:** F√°cil agregar m√°s destinos en paralelo

#### Destinos de Datos

**PostgreSQL (Staging Local)**
- Prop√≥sito: Base de datos local para desarrollo y staging
- Tablas staging: `brent_price`, `fuel_prices`, `usd_ars_rates`
- Tablas analytics: `brent_price_monthly`, `fuel_prices_monthly`, `usd_ars_rates_monthly`

**Redshift (Producci√≥n)**
- Prop√≥sito: Data warehouse en AWS para an√°lisis en producci√≥n
- Schema: `2025_sebastian_castro_schema`
- Misma estructura de tablas que PostgreSQL
- Requiere configuraci√≥n de credenciales (ver secci√≥n de Configuraci√≥n de Redshift)

#### Ejecutar el DAG

**Desde la UI de Airflow:**
1. Ir a http://localhost:8080
2. Buscar el DAG `fuel_price_etl`
3. Activar el DAG con el toggle
4. Clic en "Trigger DAG" para ejecutarlo manualmente
5. Monitorear ejecuci√≥n en la vista Graph

**Desde la terminal:**
```bash
# Trigger manual del DAG completo
docker exec airflow_scheduler airflow dags trigger fuel_price_etl

# Ejecutar tareas individuales para testing
docker exec airflow_scheduler airflow tasks test fuel_price_etl extract 2025-11-20
docker exec airflow_scheduler airflow tasks test fuel_price_etl transform 2025-11-20
docker exec airflow_scheduler airflow tasks test fuel_price_etl load_postgres 2025-11-20
docker exec airflow_scheduler airflow tasks test fuel_price_etl load_redshift 2025-11-20
```

### Comandos √ötiles de Airflow

```bash
# Listar todos los DAGs
docker exec airflow_webserver airflow dags list

# Ver estado de un DAG espec√≠fico
docker exec airflow_webserver airflow dags state fuel_price_etl

# Ver lista de ejecuciones (DAG runs)
docker exec airflow_webserver airflow dags list-runs -d fuel_price_etl

# Ejecutar una tarea espec√≠fica en modo test (no registra en Airflow)
docker exec airflow_scheduler airflow tasks test fuel_price_etl extract 2025-11-20
docker exec airflow_scheduler airflow tasks test fuel_price_etl transform 2025-11-20
docker exec airflow_scheduler airflow tasks test fuel_price_etl load_postgres 2025-11-20

# Ver logs de una tarea
docker exec airflow_scheduler airflow tasks logs fuel_price_etl extract <RUN_ID>

# Pausar/Activar un DAG
docker exec airflow_webserver airflow dags pause fuel_price_etl
docker exec airflow_webserver airflow dags unpause fuel_price_etl

# Trigger manual de un DAG
docker exec airflow_scheduler airflow dags trigger fuel_price_etl

# Ver configuraci√≥n de variables
docker exec airflow_scheduler airflow variables list
docker exec airflow_scheduler airflow variables get fuel_etl_update_all

# Setear variables
docker exec airflow_scheduler airflow variables set fuel_etl_update_all true --json
docker exec airflow_scheduler airflow variables set fuel_etl_brent_start_date "2023-01-01"

# Ver grafo de dependencias del DAG
docker exec airflow_scheduler airflow dags show fuel_price_etl
```

### Monitoreo y Logs

**Ver logs de Airflow:**
```bash
# Logs del scheduler
docker logs -f airflow_scheduler

# Logs del webserver
docker logs -f airflow_webserver

# Logs en archivos (dentro del contenedor)
docker exec airflow_scheduler ls -la /opt/airflow/logs/
```

**Verificar salud de Airflow:**
```bash
# Ver procesos corriendo
docker-compose ps

# Ver estado de salud de contenedores
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

# Verificar que Airflow puede conectarse a la BD
docker exec airflow_scheduler airflow db check
```

### Configuraci√≥n de Airflow Variables

El DAG `fuel_price_etl` utiliza las siguientes variables opcionales que se pueden configurar desde la UI de Airflow para personalizar el comportamiento del pipeline:

| Variable | Tipo | Default | Descripci√≥n |
|----------|------|---------|-------------|
| `fuel_etl_update_all` | Boolean (JSON) | `true` | Si es `true`, actualiza todos los datos de combustibles desde la API. Si es `false`, solo actualiza datos incrementales |
| `fuel_etl_brent_start_date` | String | `"2022-01-01"` | Fecha de inicio (YYYY-MM-DD) para la extracci√≥n de precios de Brent desde Yahoo Finance |

**Para configurar en Airflow UI:**
1. Acceder a http://localhost:8080
2. Ir a **Admin** ‚Üí **Variables**
3. Hacer clic en **+** (Add a new record)
4. Ingresar los datos:
   - **Key:** `fuel_etl_update_all`
   - **Val:** `true` (marcar checkbox "Is JSON" si est√° disponible)
5. Repetir para otras variables

**Para configurar desde CLI:**
```bash
# Configurar update_all (Boolean JSON)
docker exec airflow_scheduler airflow variables set fuel_etl_update_all true --json

# Configurar fecha de inicio de Brent (String)
docker exec airflow_scheduler airflow variables set fuel_etl_brent_start_date "2023-01-01"

# Ver todas las variables configuradas
docker exec airflow_scheduler airflow variables list

# Ver una variable espec√≠fica
docker exec airflow_scheduler airflow variables get fuel_etl_update_all
```

**Notas:**
- Si no se configuran estas variables, el DAG usa los valores por defecto
- Los cambios en las variables se aplican en la **pr√≥xima ejecuci√≥n** del DAG
- Para aplicar cambios inmediatamente, dispara manualmente el DAG despu√©s de modificar las variables

### Configuraci√≥n de Redshift (Opcional)

Para habilitar la carga a AWS Redshift, configura las credenciales en el archivo `.env`:

```bash
# Agregar al archivo .env
REDSHIFT_CONNECTION_STRING=postgresql://user:password@cluster.region.redshift.amazonaws.com:5439/database_name
```

**Formato de la connection string:**
```
postgresql://[usuario]:[contrase√±a]@[cluster].[region].redshift.amazonaws.com:[puerto]/[database]
```

**Ejemplo:**
```bash
REDSHIFT_CONNECTION_STRING=postgresql://admin:MyPass123@my-cluster.us-east-1.redshift.amazonaws.com:5439/pda
```

**Notas importantes sobre Redshift:**
- ‚ö†Ô∏è Si no configuras `REDSHIFT_CONNECTION_STRING`, la tarea `load_redshift` fallar√°
- ‚úÖ La tarea `load_postgres` funciona independientemente de Redshift
- üîí El schema usado es: `2025_sebastian_castro_schema`
- üìä Se crean las mismas tablas que en PostgreSQL (staging + analytics)

**Desactivar carga a Redshift temporalmente:**

Si no tienes credenciales de Redshift, puedes:
1. Desde la UI de Airflow, marcar la tarea `load_redshift` como "skipped"
2. O comentar/eliminar temporalmente la tarea del DAG

### Validaci√≥n del DAG

**Validar estructura del DAG desde CLI:**
```bash
# Ver lista de DAGs registrados
docker exec airflow_webserver airflow dags list | grep fuel

# Verificar que el DAG no tenga errores de importaci√≥n
docker exec airflow_webserver airflow dags list-import-errors

# Ver grafo de dependencias del DAG
docker exec airflow_scheduler airflow dags show fuel_price_etl
```

**Validar con tests de Python:**
```bash
# Ejecutar tests de extracci√≥n y transformaci√≥n
poetry run pytest tests/

# Ejecutar con cobertura
poetry run pytest --cov=src/fuel_price

# Verificar tipos con mypy
poetry run mypy src/fuel_price

# Verificar estilo con flake8
poetry run flake8 src/ tests/ dags/

# Formatear c√≥digo con black
poetry run black src/ tests/ dags/
```

**Probar tareas del DAG individualmente:**
```bash
# Ejecutar cada tarea en modo test (no registra en Airflow)
docker exec airflow_scheduler airflow tasks test fuel_price_etl extract 2025-11-20
docker exec airflow_scheduler airflow tasks test fuel_price_etl transform 2025-11-20
docker exec airflow_scheduler airflow tasks test fuel_price_etl load_postgres 2025-11-20
docker exec airflow_scheduler airflow tasks test fuel_price_etl load_redshift 2025-11-20
```

**Estructura esperada del DAG:**
```
extract ‚Üí transform ‚Üí [load_postgres, load_redshift]
```

Las cargas a PostgreSQL y Redshift deben ejecutarse en **paralelo** (sin dependencias entre s√≠).

##  Autor

Sebastian J. Castro - [GitHub](https://github.com/sebacastrocba)

## üìÑ Licencia

Este proyecto es de c√≥digo abierto y est√° disponible bajo la licencia MIT.
